# Text2SQL RAG Project - Interview Preparation Guide

> **Last Updated:** February 2026  
> **Project:** Multi-Source RAG + Text-to-SQL API  
> **Deployment:** AWS Lambda (Docker) with Function URLs  
> **Repository:** github.com/jndumu/text2sqlrag-project

---

## üìã Quick Navigation

1. [Executive Summary](#executive-summary)
2. [Project Overview](#project-overview)
3. [Architecture Deep-Dive](#architecture-deep-dive)
4. [Technical Interview Q&A](#technical-interview-qa)
5. [Technical Achievements](#technical-achievements)
6. [Lessons Learned](#lessons-learned)
7. [Behavioral Interview Answers](#behavioral-interview-answers)
8. [Key Technologies](#key-technologies)
9. [Code Snippets](#code-snippets)
10. [Quick Facts Cheat Sheet](#quick-facts-cheat-sheet)

---

## Executive Summary

**What I Built:**
A production-ready RAG system deployed on AWS Lambda that converts natural language to SQL queries and answers document-based questions. Features multi-level caching (S3 + Redis), intelligent query routing, and an automated CI/CD pipeline.

**Key Achievements:**
- ‚ö° **6-7x faster deployments** (30 min ‚Üí 5 min) via base image caching
- üí∞ **60% storage cost reduction** with S3 lifecycle policies
- üöÄ **70-80% API cost reduction** with multi-level caching
- ‚è±Ô∏è **100-200ms response time** for cached queries
- üì¶ **Zero manual AWS setup** - Everything auto-created

**Technology Stack:**
- **Backend:** Python 3.12, FastAPI, async/await
- **AI/ML:** OpenAI GPT-4, Embeddings, Vanna.ai, Pinecone
- **Database:** Supabase PostgreSQL (connection pooler)
- **Storage:** AWS S3 (documents), Upstash Redis (queries)
- **Deployment:** AWS Lambda (Docker), ECR, Function URLs, GitHub Actions

**Deployment Flow:**

GitHub Push ‚Üí GitHub Actions ‚Üí Docker Build ‚Üí ECR ‚Üí Lambda ‚Üí Function URL

---

## Project Overview

### The Problem
Organizations have data scattered across documents (PDFs, DOCX) and databases (PostgreSQL). Users need to query both using natural language, but existing solutions require knowing SQL or manually searching documents.

### My Solution
Built an intelligent system that:
1. **Routes queries automatically** - Detects if question needs SQL, documents, or both
2. **RAG for documents** - Uploads PDFs, chunks text (512 tokens), generates embeddings, stores in Pinecone, answers with GPT-4
3. **Text-to-SQL** - Converts "What were total sales last month?" to `SELECT SUM(total) FROM orders WHERE...`
4. **Multi-level caching** - Redis (10-20ms) + S3 (100-200ms) ‚Üí 75% cache hit rate
5. **Production deployment** - AWS Lambda with Docker, automated CI/CD, Function URLs

### Key Features
- **18 REST endpoints** - Upload, query, stats, health checks
- **5-stage RAG pipeline** - Parse ‚Üí Chunk ‚Üí Embed ‚Üí Retrieve ‚Üí Generate
- **Two-step SQL approval** - Generate ‚Üí Review ‚Üí Execute (safety)
- **Intelligent routing** - 60+ keywords detect SQL vs. document queries
- **Automatic resource creation** - ECR repos, S3 bucket, Function URL
- **Multi-level caching** - Different TTLs per operation (15min - 7 days)

---

## Architecture Deep-Dive

User Request ‚Üì Lambda Function URL (public HTTPS) ‚Üì FastAPI Application (18 endpoints) ‚Üì Query Router (60+ keywords) ‚Üì ‚îú‚îÄ‚Üí RAG Pipeline (documents) ‚îÇ ‚îú‚îÄ‚Üí Redis Cache (1h TTL) ‚îÇ ‚îú‚îÄ‚Üí S3 Cache (30d lifecycle) ‚îÇ ‚îú‚îÄ‚Üí Pinecone (vectors) ‚îÇ ‚îî‚îÄ‚Üí OpenAI (GPT-4) ‚îÇ ‚îî‚îÄ‚Üí SQL Pipeline (database) ‚îú‚îÄ‚Üí Redis Cache (15min-24h TTL) ‚îú‚îÄ‚Üí Vanna.ai (Text-to-SQL) ‚îî‚îÄ‚Üí Supabase (PostgreSQL via pooler)

### System Architecture


### Data Flow

**Document Upload:**

User uploads PDF (5 MB)
Compute SHA-256 hash ‚Üí doc_id
Check S3 cache ‚Üí Exists? Skip parsing
If cache miss: a. Parse with Docling (context-aware) b. Chunk (512 tokens, 50 overlap) ‚Üí 45 chunks c. Embed (OpenAI) ‚Üí 45 vectors (1536 dims) d. Save to S3 (document.pdf, chunks.json, embeddings.npy, metadata.json)
Upsert to Pinecone
Return: cache_hit indicator + chunk count


**Query Flow (RAG):**

User: "What is the return policy?"
Check Redis ‚Üí Hit? Return cached answer (10ms)
If miss: a. Embed query ‚Üí 1536-dim vector b. Search Pinecone ‚Üí top 3 chunks (cosine similarity) c. Assemble context (chunks + question) d. Generate answer (GPT-4) e. Cache in Redis (1h TTL)
Return answer with sources


**Query Flow (SQL):**

User: "Total sales last month?"
Check Redis ‚Üí Hit? Return SQL (10ms)
If miss: a. Vanna generates SQL (GPT-4o) b. Return SQL + query_id (pending approval)
User approves ‚Üí Execute SQL
Cache result (15min TTL)
Return query results


### S3 Cache Structure
s3://rag-cache-josie/ pdf/{doc_id}/ document.pdf (5 MB, original) chunks.json (12 KB, parsed text) embeddings.npy (270 KB, 45 chunks √ó 1536 √ó 4 bytes) metadata.json (2 KB, processing info) txt/{doc_id}/ document.txt chunks.json embeddings.npy metadata.json


**Cache Strategy: All-or-Nothing**
- Check if ALL 4 files exist
- If any missing ‚Üí Cache miss (prevents partial corruption)
- 30-day lifecycle policy ‚Üí Auto-delete old files

### Redis Cache Structure
Keys: rag:sha256(query) ‚Üí Answer (TTL: 1h) sql:sha256(query) ‚Üí SQL (TTL: 24h) sql_result:query_id ‚Üí Results (TTL: 15min) embeddings:sha256(text) ‚Üí Vector (TTL: 7d)

Example: rag:abc123... ‚Üí { "answer": "Our return policy...", "sources": ["return_policy.pdf:2", ...], "cached_at": "2026-02-09T10:30:00Z" }

---

## Technical Interview Q&A

### Deployment & CI/CD

**Q: Walk me through your deployment pipeline.**

**A:** "My CI/CD uses GitHub Actions with base image caching for 6-7x speedup:

**Stage 1: Check for Base Image Cache**
- Try pulling `ECR/lambda-python-deps:3.12` from ECR
- If exists (cache hit): Use it (saves 25-30 minutes)
- If not (cache miss): Build once with all system dependencies

**Stage 2: Build Application Image**
- Multi-stage Dockerfile:
  - Stage 1: Import UV binary (fast package manager)
  - Stage 2: Install Python deps with BuildKit cache mounts
  - Stage 3: Copy application code (minimal runtime image)
- Result: 2GB image in 5-7 minutes

**Stage 3: Automated Resource Creation**
- Auto-create ECR repos (if missing) with image scanning + encryption
- Auto-create S3 bucket with versioning + 30-day lifecycle policy"

---

## Challenges Faced

### Challenge 1: Performance with Large Documents

**Problem:** Processing 5MB+ PDFs was slow and memory-intensive.

**Solution:** 
- Used **chunking** (512 tokens, 50 overlap) to break documents into manageable pieces
- Added **parallel processing** to handle multiple chunks simultaneously
- Optimized embeddings with **Vanna.ai** for faster inference

### Challenge 2: Query Accuracy

**Problem:** RAG sometimes returned irrelevant answers.

**Solution:**
- Added **query filtering** to discard irrelevant chunks
- Used **context assembly** to combine relevant chunks into a coherent answer
- Fine-tuned the **embedding model** to improve relevance

### Challenge 3: Cost Management

**Problem:** High costs from S3 and Redis.

**Solution:**
- Implemented **S3 lifecycle policies** to auto-delete old files
- Used **multi-level caching** to reduce API calls
- Optimized **Lambda function** to reduce execution time

---

## Technical Achievements

- ‚ö° **6-7x faster deployments** (30 min ‚Üí 5 min) via base image caching
- üí∞ **60% storage cost reduction** with S3 lifecycle policies
- üöÄ **70-80% API cost reduction** with multi-level caching
- ‚è±Ô∏è **100-200ms response time** for cached queries
- üì¶ **Zero manual AWS setup** - Everything auto-created

---

## Lessons Learned

- **Performance is critical** - Even small delays add up
- **Scalability matters** - The system must handle growth
- **Cost optimization is essential** - Every byte counts
- **AI is powerful** - But requires careful tuning and validation

---

## Behavioral Interview Answers

- **"Tell me about a time you faced a challenge and overcame it."**
- **"How do you handle pressure?"**
- **"What motivates you?"**
- **"How do you work in a team?"**
- **"What do you value?"**

---

## Key Technologies

- **Backend:** Python 3.12, FastAPI, async/await
- **AI/ML:** OpenAI GPT-4, Embeddings, Vanna.ai, Pinecone
- **Database:** Supabase PostgreSQL (connection pooler)
- **Storage:** AWS S3 (documents), Upstash Redis (queries)
- **Deployment:** AWS Lambda (Docker), ECR, Function URLs, GitHub Actions

---

## Code Snippets

- **Document upload and processing**
- **Query routing and RAG**
- **SQL generation and execution**
- **Caching strategies**

---

## Quick Facts Cheat Sheet

- **Deployment:** AWS Lambda (Docker) with Function URLs
- **Repository:** github.com/jndumu/text2sqlrag-project
- **Key Features:** 18 REST endpoints, 5-stage RAG pipeline, Two-step SQL approval
- **Performance:** 6-7x faster deployments, 100-200ms response time for cached queries
- **Cost:** 60% storage cost reduction, 70-80% API cost reduction

---